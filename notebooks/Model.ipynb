{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dummy data and take a quick look around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/midsize_dummy_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "      <th>temp</th>\n",
       "      <th>duration</th>\n",
       "      <th>sequence</th>\n",
       "      <th>distance</th>\n",
       "      <th>rel_altitude</th>\n",
       "      <th>rel_soc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.570312</td>\n",
       "      <td>-0.661392</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.574219</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040666</td>\n",
       "      <td>-0.696062</td>\n",
       "      <td>0.096667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.535156</td>\n",
       "      <td>-0.693038</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092468</td>\n",
       "      <td>-1.234818</td>\n",
       "      <td>0.196667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.160156</td>\n",
       "      <td>-0.708861</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142041</td>\n",
       "      <td>-1.711705</td>\n",
       "      <td>0.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.199219</td>\n",
       "      <td>-0.724684</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186323</td>\n",
       "      <td>-3.369781</td>\n",
       "      <td>0.376667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       speed      temp  duration  sequence  distance  rel_altitude   rel_soc\n",
       "0  25.570312 -0.661392         0         1  0.000000      0.000000 -0.000000\n",
       "1  27.574219 -0.677215         5         1  0.040666     -0.696062  0.096667\n",
       "2  30.535156 -0.693038        10         1  0.092468     -1.234818  0.196667\n",
       "3  31.160156 -0.708861        15         1  0.142041     -1.711705  0.283333\n",
       "4  34.199219 -0.724684        20         1  0.186323     -3.369781  0.376667"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000000001DF71390>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000001FFC5240>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000000020002128>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x0000000020038668>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000002006E048>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000002006E080>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x00000000200D23C8>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000002010CEB8>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x00000000200AE470>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d9c5eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.hist(bins=20, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 7 columns):\n",
      "speed           100000 non-null float64\n",
      "temp            100000 non-null float64\n",
      "duration        100000 non-null int64\n",
      "sequence        100000 non-null int64\n",
      "distance        100000 non-null float64\n",
      "rel_altitude    100000 non-null float64\n",
      "rel_soc         100000 non-null float64\n",
      "dtypes: float64(5), int64(2)\n",
      "memory usage: 5.3 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "      <th>temp</th>\n",
       "      <th>duration</th>\n",
       "      <th>sequence</th>\n",
       "      <th>distance</th>\n",
       "      <th>rel_altitude</th>\n",
       "      <th>rel_soc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30.574375</td>\n",
       "      <td>1.219536</td>\n",
       "      <td>544.320280</td>\n",
       "      <td>242.88223</td>\n",
       "      <td>3.563416</td>\n",
       "      <td>-27.932265</td>\n",
       "      <td>5.776295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.407098</td>\n",
       "      <td>4.032229</td>\n",
       "      <td>334.857363</td>\n",
       "      <td>139.85154</td>\n",
       "      <td>2.026561</td>\n",
       "      <td>13.496711</td>\n",
       "      <td>3.282041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-39.849462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-49.717242</td>\n",
       "      <td>-0.243333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.941406</td>\n",
       "      <td>-0.588785</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>122.00000</td>\n",
       "      <td>1.674787</td>\n",
       "      <td>-39.941004</td>\n",
       "      <td>3.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>34.113281</td>\n",
       "      <td>2.012308</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>243.00000</td>\n",
       "      <td>3.582275</td>\n",
       "      <td>-30.538366</td>\n",
       "      <td>5.614286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43.839844</td>\n",
       "      <td>3.695444</td>\n",
       "      <td>814.000000</td>\n",
       "      <td>364.00000</td>\n",
       "      <td>5.483049</td>\n",
       "      <td>-19.006551</td>\n",
       "      <td>8.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>255.996094</td>\n",
       "      <td>10.639733</td>\n",
       "      <td>1604.000000</td>\n",
       "      <td>486.00000</td>\n",
       "      <td>6.678766</td>\n",
       "      <td>6.681012</td>\n",
       "      <td>17.954762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               speed           temp       duration      sequence  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.00000   \n",
       "mean       30.574375       1.219536     544.320280     242.88223   \n",
       "std        16.407098       4.032229     334.857363     139.85154   \n",
       "min         0.000000     -39.849462       0.000000       1.00000   \n",
       "25%        18.941406      -0.588785     256.000000     122.00000   \n",
       "50%        34.113281       2.012308     527.000000     243.00000   \n",
       "75%        43.839844       3.695444     814.000000     364.00000   \n",
       "max       255.996094      10.639733    1604.000000     486.00000   \n",
       "\n",
       "            distance   rel_altitude        rel_soc  \n",
       "count  100000.000000  100000.000000  100000.000000  \n",
       "mean        3.563416     -27.932265       5.776295  \n",
       "std         2.026561      13.496711       3.282041  \n",
       "min         0.000000     -49.717242      -0.243333  \n",
       "25%         1.674787     -39.941004       3.100000  \n",
       "50%         3.582275     -30.538366       5.614286  \n",
       "75%         5.483049     -19.006551       8.366667  \n",
       "max         6.678766       6.681012      17.954762  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key value that will later be needed is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_ids = list(data['sequence'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the flattened data to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [data[data['sequence'] == i]  for i in seq_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "      <th>temp</th>\n",
       "      <th>duration</th>\n",
       "      <th>sequence</th>\n",
       "      <th>distance</th>\n",
       "      <th>rel_altitude</th>\n",
       "      <th>rel_soc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.570312</td>\n",
       "      <td>-0.661392</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.574219</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040666</td>\n",
       "      <td>-0.696062</td>\n",
       "      <td>0.096667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.535156</td>\n",
       "      <td>-0.693038</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092468</td>\n",
       "      <td>-1.234818</td>\n",
       "      <td>0.196667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.160156</td>\n",
       "      <td>-0.708861</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142041</td>\n",
       "      <td>-1.711705</td>\n",
       "      <td>0.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.199219</td>\n",
       "      <td>-0.724684</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186323</td>\n",
       "      <td>-3.369781</td>\n",
       "      <td>0.376667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32.167969</td>\n",
       "      <td>-0.740506</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186323</td>\n",
       "      <td>-3.369781</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33.433594</td>\n",
       "      <td>-0.756329</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.227814</td>\n",
       "      <td>-3.107569</td>\n",
       "      <td>0.505556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35.300781</td>\n",
       "      <td>-0.772152</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.267211</td>\n",
       "      <td>-3.755753</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.132812</td>\n",
       "      <td>-0.787975</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.314702</td>\n",
       "      <td>-6.448910</td>\n",
       "      <td>0.396667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.683594</td>\n",
       "      <td>-0.803797</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.363916</td>\n",
       "      <td>-6.762090</td>\n",
       "      <td>0.327778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.183594</td>\n",
       "      <td>-0.819620</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.384286</td>\n",
       "      <td>-6.940597</td>\n",
       "      <td>0.383333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21.640625</td>\n",
       "      <td>-0.835443</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0.388533</td>\n",
       "      <td>-7.015486</td>\n",
       "      <td>0.496667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34.296875</td>\n",
       "      <td>-0.851266</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.385387</td>\n",
       "      <td>-7.307934</td>\n",
       "      <td>0.696667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>38.804688</td>\n",
       "      <td>-0.867089</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.387095</td>\n",
       "      <td>-7.621623</td>\n",
       "      <td>0.856667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>39.445312</td>\n",
       "      <td>-0.882911</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.394777</td>\n",
       "      <td>-7.669913</td>\n",
       "      <td>0.896667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>38.101562</td>\n",
       "      <td>-0.898734</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409293</td>\n",
       "      <td>-7.608249</td>\n",
       "      <td>0.846667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>26.511719</td>\n",
       "      <td>-0.914557</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409293</td>\n",
       "      <td>-7.608249</td>\n",
       "      <td>0.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20.085938</td>\n",
       "      <td>-0.930380</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>0.426028</td>\n",
       "      <td>-8.537628</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30.421875</td>\n",
       "      <td>-0.946203</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>0.459875</td>\n",
       "      <td>-9.131357</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>42.367188</td>\n",
       "      <td>-0.962025</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>0.514575</td>\n",
       "      <td>-8.858637</td>\n",
       "      <td>1.216667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>44.449219</td>\n",
       "      <td>-0.977848</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.591448</td>\n",
       "      <td>-8.743205</td>\n",
       "      <td>1.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>44.175781</td>\n",
       "      <td>-0.993671</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.669616</td>\n",
       "      <td>-9.779226</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>43.921875</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>0.735102</td>\n",
       "      <td>-10.998707</td>\n",
       "      <td>1.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>46.945312</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>0.815961</td>\n",
       "      <td>-12.616942</td>\n",
       "      <td>1.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>46.804688</td>\n",
       "      <td>-0.991870</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.815961</td>\n",
       "      <td>-12.616942</td>\n",
       "      <td>1.576667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>46.433594</td>\n",
       "      <td>-0.951220</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>0.896631</td>\n",
       "      <td>-13.776149</td>\n",
       "      <td>1.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>47.753906</td>\n",
       "      <td>-0.910569</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963008</td>\n",
       "      <td>-13.975162</td>\n",
       "      <td>1.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>46.953125</td>\n",
       "      <td>-0.869919</td>\n",
       "      <td>135</td>\n",
       "      <td>1</td>\n",
       "      <td>1.043653</td>\n",
       "      <td>-11.419540</td>\n",
       "      <td>1.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>45.582031</td>\n",
       "      <td>-0.829268</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>1.110063</td>\n",
       "      <td>-9.663290</td>\n",
       "      <td>2.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>48.062500</td>\n",
       "      <td>-0.788618</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>1.188441</td>\n",
       "      <td>-8.337651</td>\n",
       "      <td>2.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>44.429688</td>\n",
       "      <td>0.060403</td>\n",
       "      <td>961</td>\n",
       "      <td>1</td>\n",
       "      <td>5.830750</td>\n",
       "      <td>-44.752013</td>\n",
       "      <td>11.176667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>42.242188</td>\n",
       "      <td>0.093960</td>\n",
       "      <td>966</td>\n",
       "      <td>1</td>\n",
       "      <td>5.888241</td>\n",
       "      <td>-44.911966</td>\n",
       "      <td>10.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>33.980469</td>\n",
       "      <td>0.127517</td>\n",
       "      <td>971</td>\n",
       "      <td>1</td>\n",
       "      <td>5.948938</td>\n",
       "      <td>-46.463914</td>\n",
       "      <td>10.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>33.164062</td>\n",
       "      <td>0.161074</td>\n",
       "      <td>976</td>\n",
       "      <td>1</td>\n",
       "      <td>6.004237</td>\n",
       "      <td>-45.493594</td>\n",
       "      <td>10.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>37.328125</td>\n",
       "      <td>0.194631</td>\n",
       "      <td>981</td>\n",
       "      <td>1</td>\n",
       "      <td>6.054714</td>\n",
       "      <td>-44.753141</td>\n",
       "      <td>11.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>38.109375</td>\n",
       "      <td>0.228188</td>\n",
       "      <td>986</td>\n",
       "      <td>1</td>\n",
       "      <td>6.103956</td>\n",
       "      <td>-39.418341</td>\n",
       "      <td>11.276667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>33.031250</td>\n",
       "      <td>0.261745</td>\n",
       "      <td>991</td>\n",
       "      <td>1</td>\n",
       "      <td>6.103956</td>\n",
       "      <td>-39.418341</td>\n",
       "      <td>11.476667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>20.957031</td>\n",
       "      <td>0.295302</td>\n",
       "      <td>996</td>\n",
       "      <td>1</td>\n",
       "      <td>6.140071</td>\n",
       "      <td>-37.947781</td>\n",
       "      <td>11.556667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>18.984375</td>\n",
       "      <td>0.328859</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>6.169400</td>\n",
       "      <td>-36.440430</td>\n",
       "      <td>11.606667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>28.937500</td>\n",
       "      <td>0.362416</td>\n",
       "      <td>1006</td>\n",
       "      <td>1</td>\n",
       "      <td>6.192358</td>\n",
       "      <td>-36.889192</td>\n",
       "      <td>11.696667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>24.117188</td>\n",
       "      <td>0.395973</td>\n",
       "      <td>1011</td>\n",
       "      <td>1</td>\n",
       "      <td>6.224863</td>\n",
       "      <td>-37.709473</td>\n",
       "      <td>11.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>9.343750</td>\n",
       "      <td>0.436242</td>\n",
       "      <td>1017</td>\n",
       "      <td>1</td>\n",
       "      <td>6.249754</td>\n",
       "      <td>-39.223341</td>\n",
       "      <td>11.656667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>11.675781</td>\n",
       "      <td>0.469799</td>\n",
       "      <td>1022</td>\n",
       "      <td>1</td>\n",
       "      <td>6.258495</td>\n",
       "      <td>-39.394356</td>\n",
       "      <td>11.706667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>25.472656</td>\n",
       "      <td>0.503356</td>\n",
       "      <td>1027</td>\n",
       "      <td>1</td>\n",
       "      <td>6.272610</td>\n",
       "      <td>-39.526957</td>\n",
       "      <td>11.796667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>19.410156</td>\n",
       "      <td>0.536913</td>\n",
       "      <td>1032</td>\n",
       "      <td>1</td>\n",
       "      <td>6.289783</td>\n",
       "      <td>-40.012700</td>\n",
       "      <td>11.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.101562</td>\n",
       "      <td>0.570470</td>\n",
       "      <td>1037</td>\n",
       "      <td>1</td>\n",
       "      <td>6.304311</td>\n",
       "      <td>-40.237520</td>\n",
       "      <td>11.730303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.604027</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>6.303439</td>\n",
       "      <td>-40.288640</td>\n",
       "      <td>11.753030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.195312</td>\n",
       "      <td>0.671141</td>\n",
       "      <td>1052</td>\n",
       "      <td>1</td>\n",
       "      <td>6.303439</td>\n",
       "      <td>-40.288640</td>\n",
       "      <td>11.798485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>17.105469</td>\n",
       "      <td>0.704698</td>\n",
       "      <td>1057</td>\n",
       "      <td>1</td>\n",
       "      <td>6.303439</td>\n",
       "      <td>-40.288640</td>\n",
       "      <td>11.856667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>32.488281</td>\n",
       "      <td>0.738255</td>\n",
       "      <td>1062</td>\n",
       "      <td>1</td>\n",
       "      <td>6.325116</td>\n",
       "      <td>-40.583818</td>\n",
       "      <td>12.036667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>37.355469</td>\n",
       "      <td>0.771812</td>\n",
       "      <td>1067</td>\n",
       "      <td>1</td>\n",
       "      <td>6.360094</td>\n",
       "      <td>-41.604530</td>\n",
       "      <td>12.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>36.160156</td>\n",
       "      <td>0.805369</td>\n",
       "      <td>1072</td>\n",
       "      <td>1</td>\n",
       "      <td>6.403544</td>\n",
       "      <td>-38.975202</td>\n",
       "      <td>12.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>30.410156</td>\n",
       "      <td>0.838926</td>\n",
       "      <td>1077</td>\n",
       "      <td>1</td>\n",
       "      <td>6.433927</td>\n",
       "      <td>-37.836060</td>\n",
       "      <td>12.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>15.109375</td>\n",
       "      <td>0.872483</td>\n",
       "      <td>1082</td>\n",
       "      <td>1</td>\n",
       "      <td>6.455490</td>\n",
       "      <td>-35.546846</td>\n",
       "      <td>12.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>24.609375</td>\n",
       "      <td>0.906040</td>\n",
       "      <td>1087</td>\n",
       "      <td>1</td>\n",
       "      <td>6.486580</td>\n",
       "      <td>-36.721350</td>\n",
       "      <td>12.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>35.246094</td>\n",
       "      <td>0.939597</td>\n",
       "      <td>1092</td>\n",
       "      <td>1</td>\n",
       "      <td>6.486580</td>\n",
       "      <td>-36.721350</td>\n",
       "      <td>12.359524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>34.367188</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>1097</td>\n",
       "      <td>1</td>\n",
       "      <td>6.519128</td>\n",
       "      <td>-42.239572</td>\n",
       "      <td>12.276667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>36.398438</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1102</td>\n",
       "      <td>1</td>\n",
       "      <td>6.565187</td>\n",
       "      <td>-46.924149</td>\n",
       "      <td>12.176667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>26.753906</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1107</td>\n",
       "      <td>1</td>\n",
       "      <td>6.607437</td>\n",
       "      <td>-47.831270</td>\n",
       "      <td>12.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>8.964844</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1112</td>\n",
       "      <td>1</td>\n",
       "      <td>6.640595</td>\n",
       "      <td>-48.996459</td>\n",
       "      <td>11.922222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         speed      temp  duration  sequence  distance  rel_altitude  \\\n",
       "0    25.570312 -0.661392         0         1  0.000000      0.000000   \n",
       "1    27.574219 -0.677215         5         1  0.040666     -0.696062   \n",
       "2    30.535156 -0.693038        10         1  0.092468     -1.234818   \n",
       "3    31.160156 -0.708861        15         1  0.142041     -1.711705   \n",
       "4    34.199219 -0.724684        20         1  0.186323     -3.369781   \n",
       "5    32.167969 -0.740506        25         1  0.186323     -3.369781   \n",
       "6    33.433594 -0.756329        30         1  0.227814     -3.107569   \n",
       "7    35.300781 -0.772152        35         1  0.267211     -3.755753   \n",
       "8    31.132812 -0.787975        40         1  0.314702     -6.448910   \n",
       "9    17.683594 -0.803797        45         1  0.363916     -6.762090   \n",
       "10   12.183594 -0.819620        50         1  0.384286     -6.940597   \n",
       "11   21.640625 -0.835443        55         1  0.388533     -7.015486   \n",
       "12   34.296875 -0.851266        60         1  0.385387     -7.307934   \n",
       "13   38.804688 -0.867089        65         1  0.387095     -7.621623   \n",
       "14   39.445312 -0.882911        70         1  0.394777     -7.669913   \n",
       "15   38.101562 -0.898734        75         1  0.409293     -7.608249   \n",
       "16   26.511719 -0.914557        80         1  0.409293     -7.608249   \n",
       "17   20.085938 -0.930380        85         1  0.426028     -8.537628   \n",
       "18   30.421875 -0.946203        90         1  0.459875     -9.131357   \n",
       "19   42.367188 -0.962025        95         1  0.514575     -8.858637   \n",
       "20   44.449219 -0.977848       100         1  0.591448     -8.743205   \n",
       "21   44.175781 -0.993671       105         1  0.669616     -9.779226   \n",
       "22   43.921875 -0.500000       110         1  0.735102    -10.998707   \n",
       "23   46.945312 -0.333333       115         1  0.815961    -12.616942   \n",
       "24   46.804688 -0.991870       120         1  0.815961    -12.616942   \n",
       "25   46.433594 -0.951220       125         1  0.896631    -13.776149   \n",
       "26   47.753906 -0.910569       130         1  0.963008    -13.975162   \n",
       "27   46.953125 -0.869919       135         1  1.043653    -11.419540   \n",
       "28   45.582031 -0.829268       140         1  1.110063     -9.663290   \n",
       "29   48.062500 -0.788618       145         1  1.188441     -8.337651   \n",
       "..         ...       ...       ...       ...       ...           ...   \n",
       "172  44.429688  0.060403       961         1  5.830750    -44.752013   \n",
       "173  42.242188  0.093960       966         1  5.888241    -44.911966   \n",
       "174  33.980469  0.127517       971         1  5.948938    -46.463914   \n",
       "175  33.164062  0.161074       976         1  6.004237    -45.493594   \n",
       "176  37.328125  0.194631       981         1  6.054714    -44.753141   \n",
       "177  38.109375  0.228188       986         1  6.103956    -39.418341   \n",
       "178  33.031250  0.261745       991         1  6.103956    -39.418341   \n",
       "179  20.957031  0.295302       996         1  6.140071    -37.947781   \n",
       "180  18.984375  0.328859      1001         1  6.169400    -36.440430   \n",
       "181  28.937500  0.362416      1006         1  6.192358    -36.889192   \n",
       "182  24.117188  0.395973      1011         1  6.224863    -37.709473   \n",
       "183   9.343750  0.436242      1017         1  6.249754    -39.223341   \n",
       "184  11.675781  0.469799      1022         1  6.258495    -39.394356   \n",
       "185  25.472656  0.503356      1027         1  6.272610    -39.526957   \n",
       "186  19.410156  0.536913      1032         1  6.289783    -40.012700   \n",
       "187   1.101562  0.570470      1037         1  6.304311    -40.237520   \n",
       "188   0.000000  0.604027      1042         1  6.303439    -40.288640   \n",
       "189   0.195312  0.671141      1052         1  6.303439    -40.288640   \n",
       "190  17.105469  0.704698      1057         1  6.303439    -40.288640   \n",
       "191  32.488281  0.738255      1062         1  6.325116    -40.583818   \n",
       "192  37.355469  0.771812      1067         1  6.360094    -41.604530   \n",
       "193  36.160156  0.805369      1072         1  6.403544    -38.975202   \n",
       "194  30.410156  0.838926      1077         1  6.433927    -37.836060   \n",
       "195  15.109375  0.872483      1082         1  6.455490    -35.546846   \n",
       "196  24.609375  0.906040      1087         1  6.486580    -36.721350   \n",
       "197  35.246094  0.939597      1092         1  6.486580    -36.721350   \n",
       "198  34.367188  0.973154      1097         1  6.519128    -42.239572   \n",
       "199  36.398438  0.857143      1102         1  6.565187    -46.924149   \n",
       "200  26.753906  0.142857      1107         1  6.607437    -47.831270   \n",
       "201   8.964844  0.666667      1112         1  6.640595    -48.996459   \n",
       "\n",
       "       rel_soc  \n",
       "0    -0.000000  \n",
       "1     0.096667  \n",
       "2     0.196667  \n",
       "3     0.283333  \n",
       "4     0.376667  \n",
       "5     0.450000  \n",
       "6     0.505556  \n",
       "7     0.466667  \n",
       "8     0.396667  \n",
       "9     0.327778  \n",
       "10    0.383333  \n",
       "11    0.496667  \n",
       "12    0.696667  \n",
       "13    0.856667  \n",
       "14    0.896667  \n",
       "15    0.846667  \n",
       "16    0.783333  \n",
       "17    0.750000  \n",
       "18    0.916667  \n",
       "19    1.216667  \n",
       "20    1.316667  \n",
       "21    1.400000  \n",
       "22    1.466667  \n",
       "23    1.526667  \n",
       "24    1.576667  \n",
       "25    1.633333  \n",
       "26    1.716667  \n",
       "27    1.816667  \n",
       "28    2.116667  \n",
       "29    2.283333  \n",
       "..         ...  \n",
       "172  11.176667  \n",
       "173  10.983333  \n",
       "174  10.816667  \n",
       "175  10.916667  \n",
       "176  11.083333  \n",
       "177  11.276667  \n",
       "178  11.476667  \n",
       "179  11.556667  \n",
       "180  11.606667  \n",
       "181  11.696667  \n",
       "182  11.650000  \n",
       "183  11.656667  \n",
       "184  11.706667  \n",
       "185  11.796667  \n",
       "186  11.750000  \n",
       "187  11.730303  \n",
       "188  11.753030  \n",
       "189  11.798485  \n",
       "190  11.856667  \n",
       "191  12.036667  \n",
       "192  12.150000  \n",
       "193  12.316667  \n",
       "194  12.416667  \n",
       "195  12.316667  \n",
       "196  12.400000  \n",
       "197  12.359524  \n",
       "198  12.276667  \n",
       "199  12.176667  \n",
       "200  12.050000  \n",
       "201  11.922222  \n",
       "\n",
       "[202 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_sequences_by_length(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (list) : list of pandas.DataFrame instances to be ordered\n",
    "    Returns:\n",
    "        data (list) : the transformed data in ascending order by sequence lenght\n",
    "    \"\"\"\n",
    "    lenghts = list(map(len, data))\n",
    "    data = list(zip(lenghts, data))\n",
    "    data.sort(key = lambda tup : tup[0])\n",
    "    data = [tup[1] for tup in data]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = order_sequences_by_length(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[485])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YEESSS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [seq.drop('rel_soc', axis=1) for seq in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [pd.DataFrame(seq['rel_soc']) for seq in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out a single training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "      <th>temp</th>\n",
       "      <th>duration</th>\n",
       "      <th>sequence</th>\n",
       "      <th>distance</th>\n",
       "      <th>rel_altitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99948</th>\n",
       "      <td>31.710938</td>\n",
       "      <td>0.154812</td>\n",
       "      <td>0</td>\n",
       "      <td>486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99949</th>\n",
       "      <td>36.503906</td>\n",
       "      <td>0.149582</td>\n",
       "      <td>5</td>\n",
       "      <td>486</td>\n",
       "      <td>0.046324</td>\n",
       "      <td>-1.287285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99950</th>\n",
       "      <td>39.378906</td>\n",
       "      <td>0.144351</td>\n",
       "      <td>10</td>\n",
       "      <td>486</td>\n",
       "      <td>0.099202</td>\n",
       "      <td>-1.823284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99951</th>\n",
       "      <td>38.191406</td>\n",
       "      <td>0.139121</td>\n",
       "      <td>15</td>\n",
       "      <td>486</td>\n",
       "      <td>0.156445</td>\n",
       "      <td>-2.030406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99952</th>\n",
       "      <td>35.652344</td>\n",
       "      <td>0.133891</td>\n",
       "      <td>20</td>\n",
       "      <td>486</td>\n",
       "      <td>0.195530</td>\n",
       "      <td>-3.299744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99953</th>\n",
       "      <td>36.140625</td>\n",
       "      <td>0.128661</td>\n",
       "      <td>25</td>\n",
       "      <td>486</td>\n",
       "      <td>0.241382</td>\n",
       "      <td>-3.289248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99954</th>\n",
       "      <td>33.015625</td>\n",
       "      <td>0.123431</td>\n",
       "      <td>30</td>\n",
       "      <td>486</td>\n",
       "      <td>0.292607</td>\n",
       "      <td>-5.293124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99955</th>\n",
       "      <td>29.207031</td>\n",
       "      <td>0.118201</td>\n",
       "      <td>35</td>\n",
       "      <td>486</td>\n",
       "      <td>0.342275</td>\n",
       "      <td>-6.885946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99956</th>\n",
       "      <td>16.488281</td>\n",
       "      <td>0.112971</td>\n",
       "      <td>40</td>\n",
       "      <td>486</td>\n",
       "      <td>0.373945</td>\n",
       "      <td>-6.701737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99957</th>\n",
       "      <td>9.527344</td>\n",
       "      <td>0.107741</td>\n",
       "      <td>45</td>\n",
       "      <td>486</td>\n",
       "      <td>0.373945</td>\n",
       "      <td>-6.701737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99958</th>\n",
       "      <td>14.554688</td>\n",
       "      <td>0.102510</td>\n",
       "      <td>50</td>\n",
       "      <td>486</td>\n",
       "      <td>0.384664</td>\n",
       "      <td>-6.939451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99959</th>\n",
       "      <td>27.371094</td>\n",
       "      <td>0.097280</td>\n",
       "      <td>55</td>\n",
       "      <td>486</td>\n",
       "      <td>0.387133</td>\n",
       "      <td>-6.978449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99960</th>\n",
       "      <td>35.589844</td>\n",
       "      <td>0.092050</td>\n",
       "      <td>60</td>\n",
       "      <td>486</td>\n",
       "      <td>0.385512</td>\n",
       "      <td>-7.140718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99961</th>\n",
       "      <td>36.390625</td>\n",
       "      <td>0.086820</td>\n",
       "      <td>65</td>\n",
       "      <td>486</td>\n",
       "      <td>0.387318</td>\n",
       "      <td>-7.612838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99962</th>\n",
       "      <td>35.378906</td>\n",
       "      <td>0.081590</td>\n",
       "      <td>70</td>\n",
       "      <td>486</td>\n",
       "      <td>0.394929</td>\n",
       "      <td>-7.611887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99963</th>\n",
       "      <td>30.386719</td>\n",
       "      <td>0.076360</td>\n",
       "      <td>75</td>\n",
       "      <td>486</td>\n",
       "      <td>0.409801</td>\n",
       "      <td>-7.458011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99964</th>\n",
       "      <td>23.171875</td>\n",
       "      <td>0.071130</td>\n",
       "      <td>80</td>\n",
       "      <td>486</td>\n",
       "      <td>0.423765</td>\n",
       "      <td>-8.094209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99965</th>\n",
       "      <td>17.816406</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>85</td>\n",
       "      <td>486</td>\n",
       "      <td>0.446774</td>\n",
       "      <td>-9.099402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99966</th>\n",
       "      <td>20.875000</td>\n",
       "      <td>0.060669</td>\n",
       "      <td>90</td>\n",
       "      <td>486</td>\n",
       "      <td>0.475841</td>\n",
       "      <td>-9.210828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99967</th>\n",
       "      <td>37.058594</td>\n",
       "      <td>0.055439</td>\n",
       "      <td>95</td>\n",
       "      <td>486</td>\n",
       "      <td>0.534774</td>\n",
       "      <td>-8.735018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99968</th>\n",
       "      <td>41.906250</td>\n",
       "      <td>0.050209</td>\n",
       "      <td>100</td>\n",
       "      <td>486</td>\n",
       "      <td>0.534774</td>\n",
       "      <td>-8.735018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99969</th>\n",
       "      <td>44.406250</td>\n",
       "      <td>0.044979</td>\n",
       "      <td>105</td>\n",
       "      <td>486</td>\n",
       "      <td>0.605657</td>\n",
       "      <td>-8.946400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>43.902344</td>\n",
       "      <td>0.039749</td>\n",
       "      <td>110</td>\n",
       "      <td>486</td>\n",
       "      <td>0.666853</td>\n",
       "      <td>-9.670744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>42.707031</td>\n",
       "      <td>0.034519</td>\n",
       "      <td>115</td>\n",
       "      <td>486</td>\n",
       "      <td>0.744553</td>\n",
       "      <td>-11.209266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>41.871094</td>\n",
       "      <td>0.029289</td>\n",
       "      <td>120</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>41.687500</td>\n",
       "      <td>0.024059</td>\n",
       "      <td>125</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>43.585938</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>130</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>46.839844</td>\n",
       "      <td>0.013598</td>\n",
       "      <td>135</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>46.433594</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>140</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>43.199219</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>145</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>42.304688</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>150</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>44.210938</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>155</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>45.003906</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>160</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>44.816406</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>165</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>41.812500</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>170</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>37.945312</td>\n",
       "      <td>0.010934</td>\n",
       "      <td>175</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>34.726562</td>\n",
       "      <td>0.015905</td>\n",
       "      <td>180</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>25.804688</td>\n",
       "      <td>0.020875</td>\n",
       "      <td>185</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>17.824219</td>\n",
       "      <td>0.025845</td>\n",
       "      <td>190</td>\n",
       "      <td>486</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>-12.441780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>18.328125</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>195</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>25.386719</td>\n",
       "      <td>0.035785</td>\n",
       "      <td>200</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>38.601562</td>\n",
       "      <td>0.040755</td>\n",
       "      <td>205</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>42.539062</td>\n",
       "      <td>0.045726</td>\n",
       "      <td>210</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>43.472656</td>\n",
       "      <td>0.050696</td>\n",
       "      <td>215</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>41.187500</td>\n",
       "      <td>0.055666</td>\n",
       "      <td>220</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>41.589844</td>\n",
       "      <td>0.060636</td>\n",
       "      <td>225</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>40.394531</td>\n",
       "      <td>0.065606</td>\n",
       "      <td>230</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>37.722656</td>\n",
       "      <td>0.070577</td>\n",
       "      <td>235</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>31.289062</td>\n",
       "      <td>0.075547</td>\n",
       "      <td>240</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>19.664062</td>\n",
       "      <td>0.080517</td>\n",
       "      <td>245</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0.722656</td>\n",
       "      <td>0.085487</td>\n",
       "      <td>250</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090457</td>\n",
       "      <td>255</td>\n",
       "      <td>486</td>\n",
       "      <td>1.590128</td>\n",
       "      <td>-22.560561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           speed      temp  duration  sequence  distance  rel_altitude\n",
       "99948  31.710938  0.154812         0       486  0.000000      0.000000\n",
       "99949  36.503906  0.149582         5       486  0.046324     -1.287285\n",
       "99950  39.378906  0.144351        10       486  0.099202     -1.823284\n",
       "99951  38.191406  0.139121        15       486  0.156445     -2.030406\n",
       "99952  35.652344  0.133891        20       486  0.195530     -3.299744\n",
       "99953  36.140625  0.128661        25       486  0.241382     -3.289248\n",
       "99954  33.015625  0.123431        30       486  0.292607     -5.293124\n",
       "99955  29.207031  0.118201        35       486  0.342275     -6.885946\n",
       "99956  16.488281  0.112971        40       486  0.373945     -6.701737\n",
       "99957   9.527344  0.107741        45       486  0.373945     -6.701737\n",
       "99958  14.554688  0.102510        50       486  0.384664     -6.939451\n",
       "99959  27.371094  0.097280        55       486  0.387133     -6.978449\n",
       "99960  35.589844  0.092050        60       486  0.385512     -7.140718\n",
       "99961  36.390625  0.086820        65       486  0.387318     -7.612838\n",
       "99962  35.378906  0.081590        70       486  0.394929     -7.611887\n",
       "99963  30.386719  0.076360        75       486  0.409801     -7.458011\n",
       "99964  23.171875  0.071130        80       486  0.423765     -8.094209\n",
       "99965  17.816406  0.065900        85       486  0.446774     -9.099402\n",
       "99966  20.875000  0.060669        90       486  0.475841     -9.210828\n",
       "99967  37.058594  0.055439        95       486  0.534774     -8.735018\n",
       "99968  41.906250  0.050209       100       486  0.534774     -8.735018\n",
       "99969  44.406250  0.044979       105       486  0.605657     -8.946400\n",
       "99970  43.902344  0.039749       110       486  0.666853     -9.670744\n",
       "99971  42.707031  0.034519       115       486  0.744553    -11.209266\n",
       "99972  41.871094  0.029289       120       486  0.808368    -12.441780\n",
       "99973  41.687500  0.024059       125       486  0.808368    -12.441780\n",
       "99974  43.585938  0.018828       130       486  0.808368    -12.441780\n",
       "99975  46.839844  0.013598       135       486  0.808368    -12.441780\n",
       "99976  46.433594  0.008368       140       486  0.808368    -12.441780\n",
       "99977  43.199219  0.003138       145       486  0.808368    -12.441780\n",
       "99978  42.304688  0.222222       150       486  0.808368    -12.441780\n",
       "99979  44.210938  0.777778       155       486  0.808368    -12.441780\n",
       "99980  45.003906  0.571429       160       486  0.808368    -12.441780\n",
       "99981  44.816406  0.000994       165       486  0.808368    -12.441780\n",
       "99982  41.812500  0.005964       170       486  0.808368    -12.441780\n",
       "99983  37.945312  0.010934       175       486  0.808368    -12.441780\n",
       "99984  34.726562  0.015905       180       486  0.808368    -12.441780\n",
       "99985  25.804688  0.020875       185       486  0.808368    -12.441780\n",
       "99986  17.824219  0.025845       190       486  0.808368    -12.441780\n",
       "99987  18.328125  0.030815       195       486  1.590128    -22.560561\n",
       "99988  25.386719  0.035785       200       486  1.590128    -22.560561\n",
       "99989  38.601562  0.040755       205       486  1.590128    -22.560561\n",
       "99990  42.539062  0.045726       210       486  1.590128    -22.560561\n",
       "99991  43.472656  0.050696       215       486  1.590128    -22.560561\n",
       "99992  41.187500  0.055666       220       486  1.590128    -22.560561\n",
       "99993  41.589844  0.060636       225       486  1.590128    -22.560561\n",
       "99994  40.394531  0.065606       230       486  1.590128    -22.560561\n",
       "99995  37.722656  0.070577       235       486  1.590128    -22.560561\n",
       "99996  31.289062  0.075547       240       486  1.590128    -22.560561\n",
       "99997  19.664062  0.080517       245       486  1.590128    -22.560561\n",
       "99998   0.722656  0.085487       250       486  1.590128    -22.560561\n",
       "99999   0.000000  0.090457       255       486  1.590128    -22.560561"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rel_soc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99948</th>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99949</th>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99950</th>\n",
       "      <td>0.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99951</th>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99952</th>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99953</th>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99954</th>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99955</th>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99956</th>\n",
       "      <td>0.128571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99957</th>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99958</th>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99959</th>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99960</th>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99961</th>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99962</th>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99963</th>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99964</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99965</th>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99966</th>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99967</th>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99968</th>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99969</th>\n",
       "      <td>1.091667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>1.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>1.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>1.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>1.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>1.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>1.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>1.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>1.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>1.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>1.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>1.978571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>1.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>1.907143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>1.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>1.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>1.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>1.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>1.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>1.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>1.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>1.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>1.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>1.755556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1.644444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1.605263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1.657895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rel_soc\n",
       "99948 -0.000000\n",
       "99949  0.200000\n",
       "99950  0.283333\n",
       "99951  0.380000\n",
       "99952  0.363636\n",
       "99953  0.318182\n",
       "99954  0.262500\n",
       "99955  0.200000\n",
       "99956  0.128571\n",
       "99957  0.160000\n",
       "99958  0.260000\n",
       "99959  0.450000\n",
       "99960  0.625000\n",
       "99961  0.687500\n",
       "99962  0.620000\n",
       "99963  0.555556\n",
       "99964  0.500000\n",
       "99965  0.562500\n",
       "99966  0.680000\n",
       "99967  0.833333\n",
       "99968  0.940000\n",
       "99969  1.091667\n",
       "99970  1.050000\n",
       "99971  1.008333\n",
       "99972  1.080000\n",
       "99973  1.140000\n",
       "99974  1.190000\n",
       "99975  1.280000\n",
       "99976  1.460000\n",
       "99977  1.580000\n",
       "99978  1.666667\n",
       "99979  1.760000\n",
       "99980  1.860000\n",
       "99981  1.960000\n",
       "99982  1.978571\n",
       "99983  1.942857\n",
       "99984  1.907143\n",
       "99985  1.820000\n",
       "99986  1.733333\n",
       "99987  1.760000\n",
       "99988  1.850000\n",
       "99989  1.880000\n",
       "99990  1.830000\n",
       "99991  1.766667\n",
       "99992  1.714286\n",
       "99993  1.785714\n",
       "99994  1.755556\n",
       "99995  1.700000\n",
       "99996  1.644444\n",
       "99997  1.605263\n",
       "99998  1.631579\n",
       "99999  1.657895"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OMGOMGOMGOMG OLOLOLOLOLOLOL XDXDDXD :-------DDDDDDDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings / hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of entries in each timestep of input, e.g. data from different sensors\n",
    "input_dim = 6\n",
    "\n",
    "#The dimension of the predicted variable\n",
    "#output_dim = 1\n",
    "\n",
    "#number of neurons per lstm layer\n",
    "LSTM_units = 200\n",
    "\n",
    "#Additional model / training hyperparameters\n",
    "dropout_rate = 0.2\n",
    "test_ratio = 0.05\n",
    "batch_size = 32\n",
    "max_epochs = 200\n",
    "\n",
    "#Set random seed for reproducible train/test split\n",
    "random_seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripBatchGenerator(Sequence):\n",
    "    \"\"\"\n",
    "        A generator class to produce a single batch of sequences \n",
    "        for LSTM training\n",
    "        \n",
    "        Arguments:\n",
    "            x_set: The whole training set, python list of length m_examples.\n",
    "            A single example can be accessed in the manner x_set[example_idx]\n",
    "            and is a numpy array of shape (1, timesteps, n_features). Timesteps\n",
    "            can vary between examples.\n",
    "            \n",
    "            y_set: The labels corresponding to elements in x_set\n",
    "            \n",
    "            batch_size: The batch size to be used in training\n",
    "        \n",
    "        Outputs:\n",
    "            batch_x_tensor: Numpy array of shape (batch_size, max_timesteps_batch,\n",
    "            n_input_features)\n",
    "            batch_y_tensor: Numpy array of shape (batch_size, max_timesteps_batch,\n",
    "            n_output_features)\n",
    "                \n",
    "    \n",
    "        #https://keras.io/utils/#sequence\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            Should implement a check that n_features is the same for all examples \n",
    "        \"\"\"\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size].copy()\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size].copy()\n",
    "\n",
    "        #get all the stuff required for reshaping\n",
    "        max_timesteps_batch = max([seq.shape[1] for seq in batch_x])\n",
    "        input_dim = batch_x[0].shape[2]\n",
    "        output_dim = batch_y[0].shape[2]\n",
    "            \n",
    "        \n",
    "        #initialize return variables as 3D tensors\n",
    "        batch_x_tensor = np.zeros((len(batch_x), max_timesteps_batch, input_dim))\n",
    "        batch_y_tensor = np.zeros((len(batch_y), max_timesteps_batch, output_dim))\n",
    "        \n",
    "        #Zero pad all samples within batch to max length\n",
    "        for i in range(len(batch_x)):\n",
    "            padding_dims = ((0, 0), (0, max_timesteps_batch - batch_x[i].shape[1]), (0, 0))\n",
    "            batch_x[i] = np.pad(batch_x[i], padding_dims, 'constant', constant_values=(None, 0))\n",
    "            batch_y[i] = np.pad(batch_y[i], padding_dims, 'constant', constant_values=(None, 0))\n",
    "            \n",
    "            #Reshape to meet Keras expectation\n",
    "            batch_x[i][0] = np.reshape(batch_x[i].transpose(), (1, max_timesteps_batch, input_dim))\n",
    "            batch_y[i][0] = np.reshape(batch_y[i].transpose(), (1, max_timesteps_batch, output_dim))\n",
    "\n",
    "            #Append x, y to returnable tensor\n",
    "            batch_x_tensor[i, :, :] = batch_x[i]\n",
    "            batch_y_tensor[i, :, :] = batch_y[i]\n",
    "\n",
    "        return batch_x_tensor, batch_y_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_numpy(X, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (list) : a python list of pd.DataFrame instances\n",
    "        y (list) : a python list of pd.DataFrame instances\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "        X (list), y (list) : dataset converted to list numpy arrays of length m_examples.\n",
    "            A single example should be accessable in the manner x_set[example_idx]\n",
    "            and is a numpy array of shape (1, timesteps, n_features), where n_features is the \n",
    "            input/output dimension of the model, respectively. Furthermore, number of timesteps\n",
    "            is allowed to vary between examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = list(map(lambda ex : np.array(ex, ndmin=3), X))\n",
    "    y = list(map(lambda ex : np.array(ex, ndmin=3), y))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = dataset_to_numpy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 193, 6)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 193, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = dataset_to_numpy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the training generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TripBatchGenerator(X_train, y_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a single training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor, y_tensor = ts.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 251, 6)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 251, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work. Now the same thing for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_test = TripBatchGenerator(X_test, y_test, batch_size=len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "#LSTM Layers and dropout regularization\n",
    "regressor.add(LSTM(units = LSTM_units, return_sequences=True, \n",
    "                   input_shape = (None, input_dim)))\n",
    "regressor.add(Dropout(rate = dropout_rate))\n",
    "\n",
    "regressor.add(LSTM(units = LSTM_units, return_sequences=True))\n",
    "regressor.add(Dropout(rate = dropout_rate))\n",
    "\n",
    "regressor.add(LSTM(units = LSTM_units, return_sequences=True))\n",
    "regressor.add(Dropout(rate = dropout_rate))\n",
    "\n",
    "regressor.add(LSTM(units = LSTM_units, return_sequences=True))\n",
    "regressor.add(Dropout(rate = dropout_rate))\n",
    "\n",
    "#Linear output layer\n",
    "regressor.add(TimeDistributed(Dense(1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 11s 4s/step - loss: 1.2912 - val_loss: 1.0541\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05406, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.1063 - val_loss: 1.1681\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 1.0578 - val_loss: 1.0321\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.05406 to 1.03214, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.0228 - val_loss: 1.0219\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.03214 to 1.02186, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.9479 - val_loss: 0.9776\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.02186 to 0.97760, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.9172 - val_loss: 0.8890\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.97760 to 0.88899, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.8734 - val_loss: 0.8623\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.88899 to 0.86227, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.8427 - val_loss: 0.8222\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.86227 to 0.82216, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8317 - val_loss: 0.8148\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.82216 to 0.81475, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8120 - val_loss: 0.8225\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.8063 - val_loss: 0.7846\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.81475 to 0.78459, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.8285 - val_loss: 0.8081\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7801 - val_loss: 0.8180\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7721 - val_loss: 0.8054\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.8014 - val_loss: 0.7472\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.78459 to 0.74716, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.8211 - val_loss: 0.7829\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7630 - val_loss: 0.7851\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7702 - val_loss: 0.7835\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7422 - val_loss: 0.7444\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.74716 to 0.74442, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7218 - val_loss: 0.7177\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.74442 to 0.71772, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7686 - val_loss: 0.7208\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7365 - val_loss: 0.7138\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.71772 to 0.71383, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6913 - val_loss: 0.7035\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.71383 to 0.70350, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7105 - val_loss: 0.7488\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.7361 - val_loss: 0.6994\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.70350 to 0.69941, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6845 - val_loss: 0.7013\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6785 - val_loss: 0.7157\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6694 - val_loss: 0.6529\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.69941 to 0.65289, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7476 - val_loss: 0.7804\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7597 - val_loss: 0.8288\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7344 - val_loss: 0.7482\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6825 - val_loss: 0.7559\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6741 - val_loss: 0.7466\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6796 - val_loss: 0.7077\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6441 - val_loss: 0.6584\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6306 - val_loss: 0.6709\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6016 - val_loss: 0.6283\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.65289 to 0.62825, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5914 - val_loss: 0.5893\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.62825 to 0.58926, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5781 - val_loss: 0.5701\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.58926 to 0.57005, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.5476 - val_loss: 0.6045\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5672 - val_loss: 0.5584\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.57005 to 0.55844, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5560 - val_loss: 0.6213\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5615 - val_loss: 0.5590\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5367 - val_loss: 0.5805\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5364 - val_loss: 0.5742\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5510 - val_loss: 0.5792\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5435 - val_loss: 0.5771\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5957 - val_loss: 0.6286\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6012 - val_loss: 0.5931\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5759 - val_loss: 0.5608\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5823 - val_loss: 0.6176\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5564 - val_loss: 0.6584\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 9s 3s/step - loss: 0.5823 - val_loss: 0.5923\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5640 - val_loss: 0.5547\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.55844 to 0.55469, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.5680 - val_loss: 0.6459\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5969 - val_loss: 0.5831\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5782 - val_loss: 0.6050\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6228 - val_loss: 0.7469\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6095 - val_loss: 0.6850\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6272 - val_loss: 0.6292\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 11s 4s/step - loss: 0.5410 - val_loss: 0.5720\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5512 - val_loss: 0.5744\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5481 - val_loss: 0.5553\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5230 - val_loss: 0.6061\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5500 - val_loss: 0.5736\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5383 - val_loss: 0.5310\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.55469 to 0.53105, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5199 - val_loss: 0.5635\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4986 - val_loss: 0.5745\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5020 - val_loss: 0.5303\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.53105 to 0.53027, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4918 - val_loss: 0.5507\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5094 - val_loss: 0.5131\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.53027 to 0.51315, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.4665 - val_loss: 0.5227\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4681 - val_loss: 0.5358\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4596 - val_loss: 0.5472\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.4874 - val_loss: 0.5199\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4615 - val_loss: 0.5751\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4724 - val_loss: 0.5402\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4746 - val_loss: 0.5276\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.4743 - val_loss: 0.5547\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4752 - val_loss: 0.5326\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.4486 - val_loss: 0.5197\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4570 - val_loss: 0.5283\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.4707 - val_loss: 0.5416\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.4731 - val_loss: 0.6121\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.5488 - val_loss: 0.5452\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.5244 - val_loss: 0.6011\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.5163 - val_loss: 0.6012\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5061 - val_loss: 0.5461\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.4831 - val_loss: 0.5300\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.4944 - val_loss: 0.5465\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4808 - val_loss: 0.5106\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.51315 to 0.51065, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4749 - val_loss: 0.5522\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4666 - val_loss: 0.5779\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4504 - val_loss: 0.5483\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4618 - val_loss: 0.5023\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.51065 to 0.50229, saving model to 23-09-2018__14_56_34.h5\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4551 - val_loss: 0.5346\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4739 - val_loss: 0.5519\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4481 - val_loss: 0.5308\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4350 - val_loss: 0.5289\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4478 - val_loss: 0.5078\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291a01d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set up some useful stuff\n",
    "from datetime import datetime as dt\n",
    "\n",
    "ny = dt.now().strftime(\"%d-%m-%Y__%H_%M_%S\")\n",
    "model_outfile = f'{ny}.h5'\n",
    "\n",
    "#save_best_only = True --> model_outfile will be overwritten each time val_loss improves\n",
    "model_checkpoint = ModelCheckpoint(model_outfile, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger(f'{ny}.log')\n",
    "\n",
    "#Compile and train the model\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_absolute_error')\n",
    "regressor.fit_generator(ts, epochs=100, verbose=1, validation_data=ts_test, callbacks=[model_checkpoint, csv_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best val_loss is 0.50229"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Dropbox\\\\Projektit\\\\Sähköbussit\\\\TripPredictor_3\\\\notebooks'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive E is Storage\n",
      " Volume Serial Number is EC63-9DAD\n",
      "\n",
      " Directory of E:\\Dropbox\\Projektit\\Sähköbussit\\TripPredictor_3\\notebooks\n",
      "\n",
      "24.09.2018  09:10    <DIR>          .\n",
      "24.09.2018  09:10    <DIR>          ..\n",
      "19.09.2018  19:30    <DIR>          .ipynb_checkpoints\n",
      "23.09.2018  15:10        13 591 624 23-09-2018__14_56_34.h5\n",
      "23.09.2018  15:11             4 301 23-09-2018__14_56_34.log\n",
      "24.09.2018  09:10           140 184 Model.ipynb\n",
      "19.09.2018  16:07           196 833 TripPredictor_3.ipynb\n",
      "19.09.2018  22:34            81 077 Trippredictor_midsize_dummytest.ipynb\n",
      "               5 File(s)     14 014 019 bytes\n",
      "               3 Dir(s)  647 591 743 488 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('23-09-2018__14_56_34.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "preds = best_model.predict_generator(ts_test, steps=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 268, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity-check that MAE is actually computed correctly. It should be the mean taken across all examples and all timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_true = ts_test.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 268, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae(preds, y_true):\n",
    "    return np.mean(np.abs(preds - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5022939350598316"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mae(preds, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
